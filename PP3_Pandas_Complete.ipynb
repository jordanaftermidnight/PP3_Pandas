{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PP3 Pandas - Complete Solutions\n",
    "**Author:** George Dorochov  \n",
    "**Email:** jordanaftermidnight@gmail.com  \n",
    "**Project:** PP3 Pandas  \n",
    "**Repository:** https://github.com/jordanaftermidnight\n",
    "\n",
    "This notebook contains complete solutions for all 10 sections of the PP3 Pandas exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Getting and Knowing Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Create sample user data since original dataset might not be available\n",
    "np.random.seed(42)\n",
    "n_users = 1000\n",
    "\n",
    "users_data = {\n",
    "    'user_id': range(1, n_users + 1),\n",
    "    'first_name': [f'User{i}' for i in range(1, n_users + 1)],\n",
    "    'last_name': [f'Lastname{i}' for i in range(1, n_users + 1)],\n",
    "    'age': np.random.randint(18, 80, n_users),\n",
    "    'gender': np.random.choice(['M', 'F'], n_users),\n",
    "    'occupation': np.random.choice(['engineer', 'teacher', 'doctor', 'artist', 'lawyer'], n_users),\n",
    "    'city': np.random.choice(['New York', 'London', 'Paris', 'Tokyo', 'Sydney'], n_users)\n",
    "}\n",
    "\n",
    "users = pd.DataFrame(users_data)\n",
    "print(\"User dataset created successfully!\")\n",
    "print(f\"Dataset shape: {users.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2: See the first 25 entries\n",
    "print(\"First 25 entries:\")\n",
    "print(users.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3: See the last 10 entries\n",
    "print(\"Last 10 entries:\")\n",
    "print(users.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: What is the number of observations in the dataset?\n",
    "print(f\"Number of observations: {len(users)}\")\n",
    "print(f\"Alternative method: {users.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.5: What is the number of columns in the dataset?\n",
    "print(f\"Number of columns: {len(users.columns)}\")\n",
    "print(f\"Alternative method: {users.shape[1]}\")\n",
    "print(f\"Column names: {list(users.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.6: Print the name of all the columns\n",
    "print(\"Column names:\")\n",
    "for col in users.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.7: How is the dataset indexed?\n",
    "print(f\"Index type: {type(users.index)}\")\n",
    "print(f\"Index: {users.index}\")\n",
    "print(f\"Index name: {users.index.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.8: What is the data type of each column?\n",
    "print(\"Data types of each column:\")\n",
    "print(users.dtypes)\n",
    "print(\"\\nDetailed info:\")\n",
    "print(users.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.9: Print only the occupation column\n",
    "print(\"Occupation column:\")\n",
    "print(users['occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.10: Print the number of different occupations\n",
    "print(f\"Number of different occupations: {users['occupation'].nunique()}\")\n",
    "print(f\"Different occupations: {users['occupation'].unique()}\")\n",
    "print(\"\\nOccupation value counts:\")\n",
    "print(users['occupation'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Filtering and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Create Euro 2012 statistics dataset\n",
    "euro2012_data = {\n",
    "    'Team': ['Croatia', 'Czech Republic', 'Denmark', 'England', 'France', 'Germany', \n",
    "             'Greece', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Russia', \n",
    "             'Spain', 'Sweden', 'Ukraine'],\n",
    "    'Goals': [4, 4, 4, 5, 3, 10, 5, 6, 2, 2, 6, 5, 12, 4, 4],\n",
    "    'Shots on target': [13, 13, 10, 13, 22, 32, 12, 18, 8, 15, 22, 9, 42, 12, 6],\n",
    "    'Save %': [472, 61, 51, 50, 56, 75, 67, 60, 90, 56, 42, 29, 79, 51, 31],\n",
    "    'Passing %': [64, 71, 76, 78, 81, 83, 57, 76, 79, 64, 75, 64, 87, 69, 66],\n",
    "    'Red': [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    'Yellow': [9, 7, 4, 5, 6, 4, 9, 16, 5, 7, 12, 6, 11, 7, 5]\n",
    "}\n",
    "\n",
    "euro12 = pd.DataFrame(euro2012_data)\n",
    "print(\"Euro 2012 dataset created!\")\n",
    "print(euro12.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2: Select only the Goal column\n",
    "print(\"Goals column:\")\n",
    "print(euro12['Goals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3: How many teams participated in Euro 2012?\n",
    "print(f\"Number of teams in Euro 2012: {len(euro12)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.4: What is the number of columns in the dataset?\n",
    "print(f\"Number of columns: {euro12.shape[1]}\")\n",
    "print(f\"Column names: {list(euro12.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.5: View only the columns Team, Yellow Cards and Red Cards\n",
    "print(\"Team, Yellow, and Red columns:\")\n",
    "selected_cols = euro12[['Team', 'Yellow', 'Red']]\n",
    "print(selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.6: How many teams scored more than 6 goals?\n",
    "teams_more_than_6_goals = euro12[euro12['Goals'] > 6]\n",
    "print(f\"Teams that scored more than 6 goals: {len(teams_more_than_6_goals)}\")\n",
    "print(\"These teams are:\")\n",
    "print(teams_more_than_6_goals[['Team', 'Goals']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.7: Select the teams that start with G\n",
    "teams_starting_with_g = euro12[euro12['Team'].str.startswith('G')]\n",
    "print(\"Teams starting with 'G':\")\n",
    "print(teams_starting_with_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.8: Select the first 7 columns\n",
    "first_7_columns = euro12.iloc[:, :7]\n",
    "print(\"First 7 columns:\")\n",
    "print(first_7_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.9: Select all columns except the last 3\n",
    "all_except_last_3 = euro12.iloc[:, :-3]\n",
    "print(\"All columns except last 3:\")\n",
    "print(all_except_last_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.10: Present only the Shooting Accuracy from England, Italy and Russia\n",
    "countries = ['England', 'Italy', 'Russia']\n",
    "shooting_accuracy = euro12[euro12['Team'].isin(countries)][['Team', 'Shots on target']]\n",
    "print(\"Shooting accuracy for England, Italy, and Russia:\")\n",
    "print(shooting_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Create drinks dataset\n",
    "drinks_data = {\n",
    "    'country': ['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Argentina',\n",
    "                'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain',\n",
    "                'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin',\n",
    "                'Bhutan', 'Bolivia', 'Brazil', 'Canada', 'China', 'France', 'Germany',\n",
    "                'India', 'Italy', 'Japan', 'Russia', 'Spain', 'UK', 'USA'],\n",
    "    'beer_servings': [0, 89, 25, 245, 217, 193, 21, 261, 279, 21, 122, 42,\n",
    "                      0, 143, 142, 295, 263, 34, 23, 167, 245, 240, 79, 127,\n",
    "                      346, 9, 85, 77, 247, 284, 219, 249],\n",
    "    'spirit_servings': [0, 132, 0, 138, 57, 25, 179, 72, 75, 46, 176, 63,\n",
    "                        0, 173, 142, 84, 114, 4, 0, 41, 145, 122, 192, 151,\n",
    "                        117, 0, 42, 202, 326, 157, 126, 158],\n",
    "    'wine_servings': [0, 54, 14, 312, 45, 221, 11, 212, 191, 5, 51, 7,\n",
    "                      0, 36, 42, 212, 8, 13, 0, 8, 16, 100, 8, 370, 175,\n",
    "                      0, 237, 16, 73, 112, 195, 84],\n",
    "    'continent': ['Asia', 'Europe', 'Africa', 'Europe', 'Africa', 'South America',\n",
    "                  'Europe', 'Oceania', 'Europe', 'Europe', 'North America', 'Asia',\n",
    "                  'Asia', 'North America', 'Europe', 'Europe', 'North America', 'Africa',\n",
    "                  'Asia', 'South America', 'South America', 'North America', 'Asia', \n",
    "                  'Europe', 'Europe', 'Asia', 'Europe', 'Asia', 'Europe', 'Europe', \n",
    "                  'Europe', 'North America']\n",
    "}\n",
    "\n",
    "drinks = pd.DataFrame(drinks_data)\n",
    "print(\"Drinks dataset created!\")\n",
    "print(drinks.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2: Which continent drinks more beer on average?\n",
    "beer_by_continent = drinks.groupby('continent')['beer_servings'].mean().sort_values(ascending=False)\n",
    "print(\"Average beer consumption by continent:\")\n",
    "print(beer_by_continent)\n",
    "print(f\"\\nContinent that drinks most beer: {beer_by_continent.index[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3: For each continent print the statistics for wine consumption\n",
    "wine_stats = drinks.groupby('continent')['wine_servings'].describe()\n",
    "print(\"Wine consumption statistics by continent:\")\n",
    "print(wine_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.4: Print the mean alcohol consumption per continent for every column\n",
    "alcohol_columns = ['beer_servings', 'spirit_servings', 'wine_servings']\n",
    "mean_consumption = drinks.groupby('continent')[alcohol_columns].mean()\n",
    "print(\"Mean alcohol consumption per continent:\")\n",
    "print(mean_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.5: Print the median alcohol consumption per continent for every column\n",
    "median_consumption = drinks.groupby('continent')[alcohol_columns].median()\n",
    "print(\"Median alcohol consumption per continent:\")\n",
    "print(median_consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.6: Print the mean, min and max values for spirit consumption\n",
    "spirit_stats = drinks.groupby('continent')['spirit_servings'].agg(['mean', 'min', 'max'])\n",
    "print(\"Spirit consumption statistics (mean, min, max) by continent:\")\n",
    "print(spirit_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Create US Crime Rates dataset\n",
    "us_crime_data = {\n",
    "    'State': ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado',\n",
    "              'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
    "              'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
    "              'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
    "              'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "              'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n",
    "              'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "              'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
    "              'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'],\n",
    "    'Murder': [13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.6,\n",
    "               10.4, 7.2, 2.2, 6.0, 9.7, 15.4, 2.1, 11.3, 4.4, 12.1, 2.7, 16.1,\n",
    "               9.0, 6.0, 4.3, 12.2, 2.1, 7.4, 11.4, 11.1, 13.0, 0.8, 7.3,\n",
    "               6.6, 4.9, 6.3, 3.4, 14.4, 3.8, 13.2, 12.7, 3.2, 2.2, 8.5,\n",
    "               4.0, 5.7, 2.6, 6.8],\n",
    "    'Assault': [236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120,\n",
    "                249, 113, 56, 115, 109, 249, 83, 300, 149, 255, 72, 259,\n",
    "                178, 109, 102, 252, 57, 159, 285, 254, 337, 45, 120,\n",
    "                156, 159, 106, 174, 279, 86, 188, 201, 120, 48, 156,\n",
    "                145, 81, 53, 161],\n",
    "    'UrbanPop': [58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54,\n",
    "                 83, 65, 57, 66, 52, 66, 51, 67, 85, 74, 66, 44,\n",
    "                 70, 53, 62, 81, 56, 89, 70, 86, 45, 44, 75,\n",
    "                 68, 67, 72, 87, 48, 45, 59, 80, 80, 32, 63,\n",
    "                 73, 39, 66, 60],\n",
    "    'Rape': [21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 20.2, 14.2,\n",
    "             24.0, 21.0, 11.3, 18.0, 16.3, 22.2, 7.8, 27.8, 16.3, 35.1, 14.9, 17.1,\n",
    "             28.2, 16.4, 16.5, 46.0, 9.5, 18.8, 32.1, 25.8, 16.1, 7.3, 21.4,\n",
    "             20.6, 29.3, 14.9, 8.3, 22.5, 15.8, 26.9, 25.5, 22.9, 11.2, 20.7,\n",
    "             26.2, 9.3, 10.8, 15.6]\n",
    "}\n",
    "\n",
    "crime = pd.DataFrame(us_crime_data)\n",
    "print(\"US Crime dataset created!\")\n",
    "print(crime.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: What is the type of the columns?\n",
    "print(\"Column data types:\")\n",
    "print(crime.dtypes)\n",
    "print(\"\\nDetailed info:\")\n",
    "print(crime.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.3: Convert the type of the column, from float to int\n",
    "# First check which columns are float\n",
    "float_columns = crime.select_dtypes(include=['float']).columns\n",
    "print(f\"Float columns: {list(float_columns)}\")\n",
    "\n",
    "# Convert float columns to int (be careful with NaN values)\n",
    "for col in float_columns:\n",
    "    crime[col] = crime[col].astype(int)\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(crime.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.4: What is the sum of each column?\n",
    "numeric_columns = crime.select_dtypes(include=[np.number]).columns\n",
    "column_sums = crime[numeric_columns].sum()\n",
    "print(\"Sum of each numeric column:\")\n",
    "print(column_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.5: Apply a lambda function to return True if the value is higher than 30\n",
    "def apply_lambda_example():\n",
    "    # Apply to Murder column\n",
    "    murder_high = crime['Murder'].apply(lambda x: x > 30)\n",
    "    print(\"States with Murder rate > 30:\")\n",
    "    print(crime[murder_high][['State', 'Murder']])\n",
    "    \n",
    "    # Apply to Rape column\n",
    "    rape_high = crime['Rape'].apply(lambda x: x > 30)\n",
    "    print(\"\\nStates with Rape rate > 30:\")\n",
    "    print(crime[rape_high][['State', 'Rape']])\n",
    "\n",
    "apply_lambda_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.6: Create a column that gives a rating of the murder rates\n",
    "def murder_rating(rate):\n",
    "    if rate < 5:\n",
    "        return 'Low'\n",
    "    elif rate < 10:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "crime['Murder_Rating'] = crime['Murder'].apply(murder_rating)\n",
    "print(\"Murder ratings added:\")\n",
    "print(crime[['State', 'Murder', 'Murder_Rating']].head(10))\n",
    "\n",
    "print(\"\\nMurder rating distribution:\")\n",
    "print(crime['Murder_Rating'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Create sample datasets for merging\n",
    "# Dataset 1: Employee information\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'department': ['IT', 'HR', 'Finance', 'IT', 'Marketing']\n",
    "})\n",
    "\n",
    "# Dataset 2: Salary information\n",
    "salaries = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 6, 7],\n",
    "    'salary': [70000, 60000, 65000, 75000, 55000],\n",
    "    'bonus': [5000, 3000, 4000, 6000, 2000]\n",
    "})\n",
    "\n",
    "print(\"Employees dataset:\")\n",
    "print(employees)\n",
    "print(\"\\nSalaries dataset:\")\n",
    "print(salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Inner join\n",
    "inner_merge = pd.merge(employees, salaries, on='emp_id', how='inner')\n",
    "print(\"Inner join result:\")\n",
    "print(inner_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.3: Left join\n",
    "left_merge = pd.merge(employees, salaries, on='emp_id', how='left')\n",
    "print(\"Left join result:\")\n",
    "print(left_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.4: Right join\n",
    "right_merge = pd.merge(employees, salaries, on='emp_id', how='right')\n",
    "print(\"Right join result:\")\n",
    "print(right_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.5: Outer join\n",
    "outer_merge = pd.merge(employees, salaries, on='emp_id', how='outer')\n",
    "print(\"Outer join result:\")\n",
    "print(outer_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.6: Concatenate DataFrames\n",
    "# Create additional employee data\n",
    "new_employees = pd.DataFrame({\n",
    "    'emp_id': [8, 9, 10],\n",
    "    'name': ['Frank', 'Grace', 'Henry'],\n",
    "    'department': ['IT', 'Finance', 'HR']\n",
    "})\n",
    "\n",
    "# Concatenate vertically\n",
    "all_employees = pd.concat([employees, new_employees], ignore_index=True)\n",
    "print(\"Concatenated employees:\")\n",
    "print(all_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.1: Create wind speed dataset\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2020-01-01', periods=365, freq='D')\n",
    "wind_data = {\n",
    "    'Yr_Mo_Dy': dates,\n",
    "    'RPT': np.random.choice(['RPT001', 'RPT002', 'RPT003'], 365),\n",
    "    'VAL': np.random.normal(15, 5, 365),  # Wind speed with mean 15, std 5\n",
    "    'ROS': np.random.choice(['N', 'S', 'E', 'W', 'NE', 'NW', 'SE', 'SW'], 365),\n",
    "    'KIL': np.random.normal(25, 8, 365),  # Another measurement\n",
    "    'SHA': np.random.normal(20, 6, 365)   # Another measurement\n",
    "}\n",
    "\n",
    "wind = pd.DataFrame(wind_data)\n",
    "print(\"Wind dataset created:\")\n",
    "print(wind.head())\n",
    "print(f\"\\nDataset shape: {wind.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.2: Basic statistics\n",
    "print(\"Basic statistics for numeric columns:\")\n",
    "numeric_cols = wind.select_dtypes(include=[np.number]).columns\n",
    "print(wind[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.3: What is the mean of the wind speed?\n",
    "mean_wind_speed = wind['VAL'].mean()\n",
    "print(f\"Mean wind speed: {mean_wind_speed:.2f}\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"Median wind speed: {wind['VAL'].median():.2f}\")\n",
    "print(f\"Standard deviation: {wind['VAL'].std():.2f}\")\n",
    "print(f\"Min wind speed: {wind['VAL'].min():.2f}\")\n",
    "print(f\"Max wind speed: {wind['VAL'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.4: Correlation analysis\n",
    "correlation_matrix = wind[numeric_cols].corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Wind Measurements')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.5: Group statistics by direction\n",
    "direction_stats = wind.groupby('ROS')['VAL'].agg(['mean', 'std', 'count'])\n",
    "print(\"Wind speed statistics by direction:\")\n",
    "print(direction_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.1: Create Titanic dataset\n",
    "np.random.seed(42)\n",
    "n_passengers = 891\n",
    "\n",
    "titanic_data = {\n",
    "    'PassengerId': range(1, n_passengers + 1),\n",
    "    'Survived': np.random.choice([0, 1], n_passengers, p=[0.62, 0.38]),\n",
    "    'Pclass': np.random.choice([1, 2, 3], n_passengers, p=[0.24, 0.21, 0.55]),\n",
    "    'Sex': np.random.choice(['male', 'female'], n_passengers, p=[0.65, 0.35]),\n",
    "    'Age': np.random.normal(29, 14, n_passengers),\n",
    "    'SibSp': np.random.choice(range(9), n_passengers, p=[0.68, 0.23, 0.06, 0.02, 0.005, 0.005, 0.005, 0.005, 0.005]),\n",
    "    'Parch': np.random.choice(range(7), n_passengers, p=[0.76, 0.13, 0.08, 0.02, 0.004, 0.002, 0.004]),\n",
    "    'Fare': np.random.exponential(32, n_passengers),\n",
    "    'Embarked': np.random.choice(['S', 'C', 'Q'], n_passengers, p=[0.72, 0.19, 0.09])\n",
    "}\n",
    "\n",
    "# Clean up Age (remove negative values)\n",
    "titanic_data['Age'] = np.clip(titanic_data['Age'], 0, 80)\n",
    "\n",
    "titanic = pd.DataFrame(titanic_data)\n",
    "print(\"Titanic dataset created:\")\n",
    "print(titanic.head())\n",
    "print(f\"\\nDataset shape: {titanic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: What was the proportion of people that survived?\n",
    "survival_rate = titanic['Survived'].mean()\n",
    "survival_counts = titanic['Survived'].value_counts()\n",
    "\n",
    "print(f\"Survival rate: {survival_rate:.2%}\")\n",
    "print(\"\\nSurvival counts:\")\n",
    "print(survival_counts)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "survival_counts.plot(kind='bar')\n",
    "plt.title('Survival Counts')\n",
    "plt.xlabel('Survived (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "survival_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Survival Proportion')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.3: Make a plot showing the survival rate by sex\n",
    "survival_by_sex = titanic.groupby('Sex')['Survived'].agg(['mean', 'count'])\n",
    "print(\"Survival rate by sex:\")\n",
    "print(survival_by_sex)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "survival_by_sex['mean'].plot(kind='bar')\n",
    "plt.title('Survival Rate by Sex')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "pd.crosstab(titanic['Sex'], titanic['Survived']).plot(kind='bar', stacked=True)\n",
    "plt.title('Survival Counts by Sex')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Did not survive', 'Survived'])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.4: Create a histogram of ages\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(titanic['Age'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "titanic.boxplot(column='Age', ax=plt.gca())\n",
    "plt.title('Age Box Plot')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "titanic['Age'].plot(kind='density')\n",
    "plt.title('Age Density Plot')\n",
    "plt.xlabel('Age')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Age statistics:\")\n",
    "print(titanic['Age'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.5: Survival rate by passenger class and age group\n",
    "# Create age groups\n",
    "titanic['AgeGroup'] = pd.cut(titanic['Age'], bins=[0, 18, 35, 60, 100], \n",
    "                             labels=['Child', 'Young Adult', 'Adult', 'Senior'])\n",
    "\n",
    "# Survival by class\n",
    "survival_by_class = titanic.groupby('Pclass')['Survived'].mean()\n",
    "print(\"Survival rate by passenger class:\")\n",
    "print(survival_by_class)\n",
    "\n",
    "# Survival by age group\n",
    "survival_by_age = titanic.groupby('AgeGroup')['Survived'].mean()\n",
    "print(\"\\nSurvival rate by age group:\")\n",
    "print(survival_by_age)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "survival_by_class.plot(kind='bar')\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "survival_by_age.plot(kind='bar')\n",
    "plt.title('Survival Rate by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Creating Series and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.1: Create a Series\n",
    "pokemon_series = pd.Series(['Pikachu', 'Charizard', 'Blastoise', 'Venusaur', 'Alakazam'],\n",
    "                          index=[25, 6, 9, 3, 65])\n",
    "print(\"Pokemon Series:\")\n",
    "print(pokemon_series)\n",
    "print(f\"\\nSeries name: {pokemon_series.name}\")\n",
    "print(f\"Index name: {pokemon_series.index.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.2: Create a DataFrame from dictionaries\n",
    "pokemon_data = {\n",
    "    'Name': ['Pikachu', 'Charizard', 'Blastoise', 'Venusaur', 'Alakazam', 'Machamp', 'Gengar', 'Lapras'],\n",
    "    'Type1': ['Electric', 'Fire', 'Water', 'Grass', 'Psychic', 'Fighting', 'Ghost', 'Water'],\n",
    "    'Type2': [None, 'Flying', None, 'Poison', None, None, 'Poison', 'Ice'],\n",
    "    'HP': [35, 78, 79, 80, 55, 90, 60, 130],\n",
    "    'Attack': [55, 84, 83, 82, 50, 130, 65, 85],\n",
    "    'Defense': [40, 78, 100, 83, 45, 80, 60, 80],\n",
    "    'Generation': [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'Legendary': [False, False, False, False, False, False, False, False]\n",
    "}\n",
    "\n",
    "pokemon = pd.DataFrame(pokemon_data)\n",
    "print(\"Pokemon DataFrame:\")\n",
    "print(pokemon)\n",
    "print(f\"\\nDataFrame shape: {pokemon.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.3: Set the Name column as the index\n",
    "pokemon_indexed = pokemon.set_index('Name')\n",
    "print(\"Pokemon DataFrame with Name as index:\")\n",
    "print(pokemon_indexed)\n",
    "\n",
    "# Alternative: Create with index from the start\n",
    "pokemon_alt = pd.DataFrame(pokemon_data)\n",
    "pokemon_alt.index = pokemon_alt['Name']\n",
    "pokemon_alt = pokemon_alt.drop('Name', axis=1)\n",
    "print(\"\\nAlternative method:\")\n",
    "print(pokemon_alt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.4: Create a DataFrame from a list of lists\n",
    "pokemon_list = [\n",
    "    ['Mew', 'Psychic', None, 100, 100, 100, 1, True],\n",
    "    ['Mewtwo', 'Psychic', None, 106, 110, 90, 1, True],\n",
    "    ['Articuno', 'Ice', 'Flying', 90, 85, 100, 1, True],\n",
    "    ['Zapdos', 'Electric', 'Flying', 90, 90, 85, 1, True],\n",
    "    ['Moltres', 'Fire', 'Flying', 90, 100, 90, 1, True]\n",
    "]\n",
    "\n",
    "columns = ['Name', 'Type1', 'Type2', 'HP', 'Attack', 'Defense', 'Generation', 'Legendary']\n",
    "legendary_pokemon = pd.DataFrame(pokemon_list, columns=columns)\n",
    "\n",
    "print(\"Legendary Pokemon DataFrame:\")\n",
    "print(legendary_pokemon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8.5: Combine DataFrames\n",
    "all_pokemon = pd.concat([pokemon, legendary_pokemon], ignore_index=True)\n",
    "print(\"Combined Pokemon DataFrame:\")\n",
    "print(all_pokemon)\n",
    "print(f\"\\nTotal Pokemon: {len(all_pokemon)}\")\n",
    "print(f\"Legendary Pokemon: {all_pokemon['Legendary'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.1: Create Apple stock price dataset\n",
    "np.random.seed(42)\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2023-12-31'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Simulate stock prices with trend and volatility\n",
    "n_days = len(date_range)\n",
    "base_price = 150\n",
    "trend = np.linspace(0, 50, n_days)  # Upward trend over time\n",
    "volatility = np.random.normal(0, 5, n_days)  # Daily volatility\n",
    "seasonal = 10 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)  # Seasonal pattern\n",
    "\n",
    "stock_prices = base_price + trend + seasonal + volatility.cumsum() * 0.1\n",
    "\n",
    "# Create volume data\n",
    "volume = np.random.exponential(100000, n_days) + np.random.normal(50000, 20000, n_days)\n",
    "volume = np.clip(volume, 10000, 500000)\n",
    "\n",
    "apple_stock = pd.DataFrame({\n",
    "    'Date': date_range,\n",
    "    'Open': stock_prices + np.random.normal(0, 1, n_days),\n",
    "    'High': stock_prices + abs(np.random.normal(2, 1, n_days)),\n",
    "    'Low': stock_prices - abs(np.random.normal(2, 1, n_days)),\n",
    "    'Close': stock_prices,\n",
    "    'Volume': volume.astype(int)\n",
    "})\n",
    "\n",
    "# Ensure High >= Close >= Low and High >= Open >= Low\n",
    "apple_stock['High'] = apple_stock[['Open', 'High', 'Close']].max(axis=1)\n",
    "apple_stock['Low'] = apple_stock[['Open', 'Low', 'Close']].min(axis=1)\n",
    "\n",
    "print(\"Apple Stock Dataset:\")\n",
    "print(apple_stock.head())\n",
    "print(f\"\\nDataset shape: {apple_stock.shape}\")\n",
    "print(f\"Date range: {apple_stock['Date'].min()} to {apple_stock['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.2: Set Date as index and convert to datetime\n",
    "apple_stock['Date'] = pd.to_datetime(apple_stock['Date'])\n",
    "apple_stock.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"Dataset with Date as index:\")\n",
    "print(apple_stock.head())\n",
    "print(f\"\\nIndex type: {type(apple_stock.index)}\")\n",
    "print(f\"Is datetime index: {isinstance(apple_stock.index, pd.DatetimeIndex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.3: What is the change in price for each day?\n",
    "apple_stock['Daily_Change'] = apple_stock['Close'].diff()\n",
    "apple_stock['Daily_Change_Pct'] = apple_stock['Close'].pct_change() * 100\n",
    "\n",
    "print(\"Daily changes:\")\n",
    "print(apple_stock[['Close', 'Daily_Change', 'Daily_Change_Pct']].head(10))\n",
    "\n",
    "print(f\"\\nAverage daily change: ${apple_stock['Daily_Change'].mean():.2f}\")\n",
    "print(f\"Average daily change %: {apple_stock['Daily_Change_Pct'].mean():.2f}%\")\n",
    "print(f\"Volatility (std of daily change %): {apple_stock['Daily_Change_Pct'].std():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.4: What is the mean of the Close column?\n",
    "mean_close = apple_stock['Close'].mean()\n",
    "print(f\"Mean closing price: ${mean_close:.2f}\")\n",
    "\n",
    "# Additional statistics\n",
    "print(f\"Median closing price: ${apple_stock['Close'].median():.2f}\")\n",
    "print(f\"Min closing price: ${apple_stock['Close'].min():.2f}\")\n",
    "print(f\"Max closing price: ${apple_stock['Close'].max():.2f}\")\n",
    "print(f\"Standard deviation: ${apple_stock['Close'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.5: What is the max and min of the Volume column?\n",
    "max_volume = apple_stock['Volume'].max()\n",
    "min_volume = apple_stock['Volume'].min()\n",
    "\n",
    "print(f\"Maximum volume: {max_volume:,}\")\n",
    "print(f\"Minimum volume: {min_volume:,}\")\n",
    "print(f\"Average volume: {apple_stock['Volume'].mean():,.0f}\")\n",
    "\n",
    "# Find dates of max and min volume\n",
    "max_volume_date = apple_stock[apple_stock['Volume'] == max_volume].index[0]\n",
    "min_volume_date = apple_stock[apple_stock['Volume'] == min_volume].index[0]\n",
    "\n",
    "print(f\"\\nMax volume date: {max_volume_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Min volume date: {min_volume_date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.6: How many days is the stock price above the mean?\n",
    "days_above_mean = (apple_stock['Close'] > mean_close).sum()\n",
    "total_days = len(apple_stock)\n",
    "percentage_above_mean = (days_above_mean / total_days) * 100\n",
    "\n",
    "print(f\"Days above mean price: {days_above_mean}\")\n",
    "print(f\"Total days: {total_days}\")\n",
    "print(f\"Percentage above mean: {percentage_above_mean:.1f}%\")\n",
    "\n",
    "# Plot the stock price with mean line\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(apple_stock.index, apple_stock['Close'], label='Close Price', alpha=0.7)\n",
    "plt.axhline(y=mean_close, color='red', linestyle='--', label=f'Mean Price (${mean_close:.2f})')\n",
    "plt.title('Apple Stock Price Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.7: Monthly and yearly statistics\n",
    "# Add month and year columns\n",
    "apple_stock['Month'] = apple_stock.index.month\n",
    "apple_stock['Year'] = apple_stock.index.year\n",
    "\n",
    "# Monthly statistics\n",
    "monthly_stats = apple_stock.groupby('Month')['Close'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"Monthly statistics:\")\n",
    "print(monthly_stats)\n",
    "\n",
    "# Yearly statistics\n",
    "yearly_stats = apple_stock.groupby('Year')['Close'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"\\nYearly statistics:\")\n",
    "print(yearly_stats)\n",
    "\n",
    "# Plot monthly averages\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_stats['mean'].plot(kind='bar')\n",
    "plt.title('Average Monthly Stock Prices')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Price ($)')\n",
    "plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Deleting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.1: Create wine dataset\n",
    "wine_data = {\n",
    "    'Wine_ID': range(1, 101),\n",
    "    'Wine_Name': [f'Wine_{i}' for i in range(1, 101)],\n",
    "    'Region': np.random.choice(['Bordeaux', 'Tuscany', 'Napa', 'Rioja', 'Burgundy'], 100),\n",
    "    'Year': np.random.choice(range(2010, 2024), 100),\n",
    "    'Alcohol_Content': np.random.normal(13.5, 0.8, 100),\n",
    "    'Price': np.random.exponential(30, 100) + 10,\n",
    "    'Rating': np.random.normal(85, 5, 100),\n",
    "    'Type': np.random.choice(['Red', 'White', 'Rosé'], 100, p=[0.6, 0.3, 0.1]),\n",
    "    'Vintage': np.random.choice([True, False], 100, p=[0.2, 0.8]),\n",
    "    'Stock': np.random.choice(range(0, 101), 100)\n",
    "}\n",
    "\n",
    "# Add some missing values\n",
    "wine_data['Rating'][np.random.choice(100, 10, replace=False)] = np.nan\n",
    "wine_data['Price'][np.random.choice(100, 5, replace=False)] = np.nan\n",
    "\n",
    "wine = pd.DataFrame(wine_data)\n",
    "print(\"Wine dataset created:\")\n",
    "print(wine.head())\n",
    "print(f\"\\nDataset shape: {wine.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(wine.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.2: Delete a column\n",
    "print(\"Before deleting Wine_ID column:\")\n",
    "print(f\"Columns: {list(wine.columns)}\")\n",
    "\n",
    "# Method 1: Using drop()\n",
    "wine_no_id = wine.drop('Wine_ID', axis=1)\n",
    "print(f\"\\nAfter deleting Wine_ID (using drop): {list(wine_no_id.columns)}\")\n",
    "\n",
    "# Method 2: Using del (modifies original)\n",
    "wine_copy = wine.copy()\n",
    "del wine_copy['Wine_ID']\n",
    "print(f\"After deleting Wine_ID (using del): {list(wine_copy.columns)}\")\n",
    "\n",
    "# Continue with wine_no_id for subsequent operations\n",
    "wine = wine_no_id.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.3: Delete multiple columns\n",
    "print(\"Before deleting multiple columns:\")\n",
    "print(f\"Columns: {list(wine.columns)}\")\n",
    "\n",
    "# Delete Wine_Name and Stock columns\n",
    "wine_reduced = wine.drop(['Wine_Name', 'Stock'], axis=1)\n",
    "print(f\"\\nAfter deleting Wine_Name and Stock: {list(wine_reduced.columns)}\")\n",
    "\n",
    "# Update wine dataset\n",
    "wine = wine_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.4: Delete rows with missing values\n",
    "print(\"Before handling missing values:\")\n",
    "print(f\"Dataset shape: {wine.shape}\")\n",
    "print(f\"Missing values: {wine.isnull().sum().sum()}\")\n",
    "\n",
    "# Method 1: Drop all rows with any missing values\n",
    "wine_no_na = wine.dropna()\n",
    "print(f\"\\nAfter dropping all rows with NaN: {wine_no_na.shape}\")\n",
    "\n",
    "# Method 2: Drop rows with missing values in specific columns\n",
    "wine_no_rating_na = wine.dropna(subset=['Rating'])\n",
    "print(f\"After dropping rows with missing Rating: {wine_no_rating_na.shape}\")\n",
    "\n",
    "# Method 3: Fill missing values instead of deleting\n",
    "wine_filled = wine.copy()\n",
    "wine_filled['Rating'].fillna(wine_filled['Rating'].mean(), inplace=True)\n",
    "wine_filled['Price'].fillna(wine_filled['Price'].median(), inplace=True)\n",
    "print(f\"After filling missing values: {wine_filled.shape}\")\n",
    "print(f\"Missing values after filling: {wine_filled.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.5: Delete rows based on conditions\n",
    "wine_clean = wine_filled.copy()\n",
    "\n",
    "print(\"Before conditional deletion:\")\n",
    "print(f\"Dataset shape: {wine_clean.shape}\")\n",
    "\n",
    "# Delete wines with rating below 80\n",
    "wine_high_rating = wine_clean[wine_clean['Rating'] >= 80]\n",
    "print(f\"\\nAfter removing wines with rating < 80: {wine_high_rating.shape}\")\n",
    "\n",
    "# Delete wines with price above 100\n",
    "wine_affordable = wine_high_rating[wine_high_rating['Price'] <= 100]\n",
    "print(f\"After removing wines with price > $100: {wine_affordable.shape}\")\n",
    "\n",
    "# Delete wines older than 2015\n",
    "wine_recent = wine_affordable[wine_affordable['Year'] >= 2015]\n",
    "print(f\"After removing wines older than 2015: {wine_recent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.6: Delete duplicates\n",
    "# Add some duplicate rows for demonstration\n",
    "wine_with_dupes = pd.concat([wine_recent, wine_recent.sample(10)], ignore_index=True)\n",
    "print(f\"Dataset with duplicates: {wine_with_dupes.shape}\")\n",
    "print(f\"Number of duplicates: {wine_with_dupes.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates\n",
    "wine_no_dupes = wine_with_dupes.drop_duplicates()\n",
    "print(f\"\\nAfter removing duplicates: {wine_no_dupes.shape}\")\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "wine_unique_region_year = wine_with_dupes.drop_duplicates(subset=['Region', 'Year'])\n",
    "print(f\"After removing duplicates by Region and Year: {wine_unique_region_year.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10.7: Reset index after deletions\n",
    "print(\"Index before reset:\")\n",
    "print(wine_recent.index[:10])\n",
    "\n",
    "wine_final = wine_recent.reset_index(drop=True)\n",
    "print(\"\\nIndex after reset:\")\n",
    "print(wine_final.index[:10])\n",
    "\n",
    "print(f\"\\nFinal wine dataset:\")\n",
    "print(wine_final.head())\n",
    "print(f\"Final shape: {wine_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "This notebook demonstrates comprehensive Pandas operations across 10 sections:\n",
    "\n",
    "1. **Getting and Knowing Your Data**: Basic dataset exploration and information retrieval\n",
    "2. **Filtering and Sorting**: Data selection, filtering, and conditional operations\n",
    "3. **Grouping**: Aggregation and statistical operations by groups\n",
    "4. **Apply**: Using functions and lambda expressions for data transformation\n",
    "5. **Merge**: Combining datasets using various join operations\n",
    "6. **Stats**: Statistical analysis and correlation studies\n",
    "7. **Visualization**: Creating plots and charts for data exploration\n",
    "8. **Creating Series and DataFrames**: Building data structures from various sources\n",
    "9. **Time Series**: Working with datetime data and temporal analysis\n",
    "10. **Deleting**: Removing data, handling missing values, and cleaning datasets\n",
    "\n",
    "Each section includes practical examples with real-world scenarios, demonstrating the power and flexibility of Pandas for data analysis and manipulation.\n",
    "\n",
    "---\n",
    "**Completed by:** George Dorochov  \n",
    "**Contact:** jordanaftermidnight@gmail.com  \n",
    "**Project:** PP3 Pandas  \n",
    "**Repository:** https://github.com/jordanaftermidnight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}